{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFWAEdYIHFOBsKfLDnHsTj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aksh002/Data_Analatics_Course_Notes/blob/main/DS_Course7(cont_).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qyF-t_m45cN"
      },
      "outputs": [],
      "source": [
        "# Week 5\n",
        "\n",
        "# This is the final section of week 5, to see all notes go to - \"https://colab.research.google.com/drive/1FEb_if15QwuKxHWO8yKmkrT-AEhr4TeO?usp=drive_link\" with main email id\n",
        "\n",
        "#                             GRid Search\n",
        "# Used to estimate perfect hyperparameter alpha to be used to get best ridge regression(which avoid underfitting and overfitting)\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "parameter1= [{'alpha':[0.001,0.01,0.1,1,10,100,1000,10000,100000]}]\n",
        "# Its a dictionary containing all the possible values of alpha\n",
        "RR=Ridge()\n",
        "# Ridge regression object\n",
        "Grid1=GridSearchCV(RR,parameters1,cv=4)\n",
        "#Creating the gridsearch object, inputs being ridge reg obj, parameter values and no. of folds\n",
        "Grid1.fit(x_data[['horsepower','curb-weight','engine-size','highway-mpg']],y_data)\n",
        "Grid1.best_estimator_\n",
        "# We cana findthe best values for the free perameter using this atribute\n",
        "\n",
        "# We can also get the mean score of the validation data using attribute-\n",
        "scores=Grid1.cv_results_\n",
        "scores['mean_test_score']\n",
        "\n",
        "# One advantage of the grid seach is how fast we can test multile parameters . Like, Ridge regrrssion also has a option to normalise the data.\n",
        "# Code will be entirely same, only difference will be -\n",
        "parameter1= [{'alpha':[0.001,0.01,0.1,1,10,100,1000,10000,100000],'normalise':[True,False]}]\n",
        "# And results that are stored in scores are i nthe format of dictionary(see notes for table depection)\n",
        "\n",
        "# We can also print out the score for different free parameter values\n",
        "for param,mean_val,mean_test in zip(scores['params'],scores['mean_test_score'],scores['mean_train_score']):\n",
        "  print(param,\"R^2 on test data:\",mean_val,\"R^2 on train data:\",mean_test)\n",
        "# If you hv kept normalisation conditon also, it will show that too\n",
        "# See screenshot on coursera for output image\n",
        "\n",
        "#                                Week Summary\n",
        "\n",
        "# How to use cross-validation by splitting the data into folds where you use some of the folds as a training set, which we use to train the model, and the remaining parts are used as a test set, which we use to test the model. You iterate through the folds until you use each partition for training and testing. At the end, you average results as the estimate of out-of-sample error.\n",
        "# How to pick the best polynomial order and problems that arise when selecting the wrong order polynomial by analyzing models that underfit and overfit your data.\n",
        "# Select the best order of a polynomial to fit your data by minimizing the test error using a graph comparing the mean square error to the order of the fitted polynomials.\n",
        "\n",
        "# You should use ridge regression when there is a strong relationship among the independent variables.\n",
        "# That ridge regression prevents overfitting.\n",
        "# Ridge regression controls the magnitude of polynomial coefficients by introducing a hyperparameter, alpha.\n",
        "# To determine alpha, you divide your data into training  and validation data. Starting with a small value for alpha, you train the model, make a prediction using the validation data, then calculate the R-squared and store the values. You repeat the value for a larger value of alpha. You repeat the process for different alpha values, training the model, and making a prediction. You select the value of alpha that maximizes R-squared.\n",
        "\n",
        "# That grid search allows you to scan through multiple hyperparameters using the Scikit-learn library, which iterates over these parameters using cross-validation. Based on the results of the grid search method, you select optimum hyperparameter values.\n",
        "# The GridSearchCV() method takes in a dictionary as its argument where the key is the name of the hyperparameter, and the values are the hyperparameter values you wish to iterate over.\n",
        "\n",
        "\n",
        "#                                 Lab Observation :-\n",
        "\n",
        "# To get all the numeric data\n",
        "df=df._get_numeric_data()\n",
        "df.head()\n",
        "\n",
        "# To find max\n",
        "xmax=max([xtrain.values.max(), xtest.values.max()])\n",
        "\n",
        "# To make an array of values bw 2 values with particular distance\n",
        "x=np.arange(xmin, xmax, 0.1)\n",
        "\n",
        "# Making a 2D array/df(size= N X 1) out of 1D array(size= 1 X N) obtained above\n",
        "x.reshape(-1, 1)\n",
        "'''\n",
        "    It converts array like : [1,2,3,4,5,6] to : [[1]\n",
        "                                                 [2]\n",
        "                                                 [3]\n",
        "                                                 [4]\n",
        "                                                 [5]\n",
        "                                                 [6]]\n",
        "\n",
        "'''\n",
        "\n",
        "# New way to plot\n",
        "plt.plot(xtrain, y_train, 'ro', label='Training Data')\n",
        "    # the string 'ro' is a format string that specifies the style of the plot. Hereâ€™s a breakdown of what each character in the format string means:\n",
        "        #'r': This specifies the color of the markers. In this case, 'r' stands for red.\n",
        "        #'o': This specifies the marker style. The character 'o' indicates that circular markers should be used\n",
        "\n",
        "# An important code snippet\n",
        "def PollyPlot(xtrain, xtest, y_train, y_test, lr,poly_transform):\n",
        "    width = 12\n",
        "    height = 10\n",
        "    plt.figure(figsize=(width, height))\n",
        "\n",
        "\n",
        "    #training data\n",
        "    #testing data\n",
        "    # lr:  linear regression object\n",
        "    #poly_transform:  polynomial transformation object\n",
        "\n",
        "    xmax=max([xtrain.values.max(), xtest.values.max()])\n",
        "\n",
        "    xmin=min([xtrain.values.min(), xtest.values.min()])\n",
        "\n",
        "    x=np.arange(xmin, xmax, 0.1)\n",
        "\n",
        "\n",
        "    plt.plot(xtrain, y_train, 'ro', label='Training Data')\n",
        "\n",
        "    plt.plot(xtest, y_test, 'go', label='Test Data')\n",
        "    plt.plot(x, lr.predict(poly_transform.fit_transform(x.reshape(-1, 1))), label='Predicted Function')\n",
        "    plt.ylim([-10000, 60000])\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "\n",
        "# NOTE :- Always remember to drop the target variable frm the data u r gonna use at X, basiocally data processed to make predcition\n",
        "\n",
        "# Tzo returning the size of the\n",
        "x_test.shape[0]\n",
        "# shape attribute of an array returns a tuple representing the dimensions of the array. shape[0] will return the number of rows, which r basically no. og samples\n",
        "\n",
        "# NOTE :-  A negative R^2 is a sign of overfitting.\n",
        "\n",
        "# To add text at perticular position at\n",
        "plt.text(3, 0.75, 'Maximum R^2 ')\n",
        "\n",
        "# create polynomial features using the single attribute, \"CPU_frequency\". You need to evaluate the R^2 scores of the model created using different degrees of polynomial features, ranging from 1 to 5. Save this set of values of R^2 score as a list\n",
        "# Even though we hv single atribute we will use the multiple attribute way because 'poly1d' object has no attribute 'score', linearRegression does\n",
        "lre = LinearRegression()\n",
        "Rsqu_test = []\n",
        "order = [1, 2, 3, 4, 5]\n",
        "for n in order:\n",
        "    pr = PolynomialFeatures(degree=n)\n",
        "    x_train_pr = pr.fit_transform(x_train[['CPU_frequency']])\n",
        "    x_test_pr = pr.fit_transform(x_test[['CPU_frequency']])\n",
        "    lre.fit(x_train_pr, y_train)\n",
        "    Rsqu_test.append(lre.score(x_test_pr, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNFZKChB13LH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}